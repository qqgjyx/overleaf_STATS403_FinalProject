@online{bulatovRecurrentMemoryTransformer2022,
  title = {Recurrent Memory Transformer},
  author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
  date = {2022-12-08},
  eprint = {2207.06881},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.06881},
  url = {http://arxiv.org/abs/2207.06881},
  urldate = {2025-03-25},
  abstract = {Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/Users/juntangwang/Zotero/storage/VY7DVIHD/Bulatov et al. - 2022 - Recurrent memory transformer.pdf;/Users/juntangwang/Zotero/storage/VCYS8D55/2207.html}
}

@online{caronEmergingPropertiesSelfsupervised2021,
  title = {Emerging Properties in Self-Supervised Vision Transformers},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  eprint = {2104.14294},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.14294},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2025-03-25},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/juntangwang/Zotero/storage/SWZ73RWH/Caron et al. - 2021 - Emerging properties in self-supervised vision transformers.pdf;/Users/juntangwang/Zotero/storage/7EIVQ2PW/2104.html}
}

@inproceedings{darcetVisionTransformersNeed2023,
  title = {Vision Transformers Need Registers},
  author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=2dnO3LLiJ1},
  urldate = {2025-03-25},
  abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/juntangwang/Zotero/storage/I3Z6DDSM/Darcet et al. - 2023 - Vision transformers need registers.pdf}
}

@online{darcetVisionTransformersNeed2024,
  title = {Vision Transformers Need Registers},
  author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  date = {2024-04-12},
  eprint = {2309.16588},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16588},
  url = {http://arxiv.org/abs/2309.16588},
  urldate = {2025-03-25},
  abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/juntangwang/Zotero/storage/SBMI9JNR/Darcet et al. - 2024 - Vision transformers need registers.pdf;/Users/juntangwang/Zotero/storage/MDI6SXEC/2309.html}
}

@software{ilharco_gabriel_2021_5143773,
  title = {{{OpenCLIP}}},
  author = {Ilharco, Gabriel and Wortsman, Mitchell and Wightman, Ross and Gordon, Cade and Carlini, Nicholas and Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Namkoong, Hongseok and Miller, John and Hajishirzi, Hannaneh and Farhadi, Ali and Schmidt, Ludwig},
  date = {2021-07},
  doi = {10.5281/zenodo.5143773},
  url = {https://doi.org/10.5281/zenodo.5143773},
  organization = {Zenodo},
  version = {0.1},
  keywords = {notion}
}

@inproceedings{locatelloObjectCentricLearningSlot2020,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  date = {2020},
  volume = {33},
  pages = {11525--11538},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html},
  urldate = {2025-04-24},
  abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
  file = {/Users/juntangwang/Zotero/storage/C86ZXMCQ/Locatello et al. - 2020 - Object-Centric Learning with Slot Attention.pdf}
}

@article{loweDistinctiveImageFeatures2004,
  title = {Distinctive {{Image Features}} from {{Scale-Invariant Keypoints}}},
  author = {Lowe, David G.},
  date = {2004-11},
  journaltitle = {IJCV},
  shortjournal = {IJCV},
  volume = {60},
  number = {2},
  pages = {91--110},
  issn = {0920-5691},
  doi = {10.1023/B:VISI.0000029664.99615.94},
  url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
  urldate = {2025-03-25},
  abstract = {This paper presents a method for extracting distinctive invariant features from imagesthatcanbeusedtoperformreliablematchingbetweendifferent viewsof an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features toadatabase of features fromknown objects using afast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to asingle object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  langid = {english},
  keywords = {notion},
  file = {/Users/juntangwang/Zotero/storage/KLIX29VI/Lowe - 2004 - Distinctive image features from scale-invariant keypoints.pdf}
}

@online{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning}} Robust Visual Features without Supervision},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  date = {2024-02-02},
  eprint = {2304.07193},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.07193},
  url = {http://arxiv.org/abs/2304.07193},
  urldate = {2025-03-25},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/juntangwang/Zotero/storage/PDPKXRSY/Oquab et al. - 2024 - DINOv2 Learning robust visual features without supervision.pdf;/Users/juntangwang/Zotero/storage/6CCH9ZXR/2304.html}
}

@article{ReproducibilityStudyVision2025,
  title = {Reproducibility {{Study}} of “{{Vision Transformers Need Registers}}”},
  date = {2025-02-26},
  journaltitle = {Transactions on Machine Learning Research},
  url = {https://openreview.net/forum?id=w9pgM58H05},
  urldate = {2025-04-24},
  abstract = {Vision Transformers (ViTs) have achieved State-Of-The-Art (SOTA) performance in nu- merous tasks. However, the emergence of high-norm artifact tokens in supervised and self-supervised ViTs hinders interpretability of attention maps of such models. This study reproduces and validates previous work (5) addressing this issue through the use of register tokens - learnable placeholders added to the input sequence - that mitigate artifacts and yield smoother feature maps. We evaluated the presence of artifacts in various ViT models, namely DeiT-III and DINOv2 architectures, and investigated the impact of fine-tuning pre- trained ViTs with register tokens and additional regularization introduced. By conducting experiments on pre-trained and fine-tuned models, we confirm that register tokens eliminate artifact and improve attention map interpretability.},
  langid = {english},
  file = {/Users/juntangwang/Zotero/storage/KC5XX62W/2025 - Reproducibility Study of “Vision Transformers Need Registers”.pdf}
}

@online{simeoniLocalizingObjectsSelfsupervised2021,
  title = {Localizing Objects with Self-Supervised Transformers and No Labels},
  author = {Siméoni, Oriane and Puy, Gilles and Vo, Huy V. and Roburin, Simon and Gidaris, Spyros and Bursuc, Andrei and Pérez, Patrick and Marlet, Renaud and Ponce, Jean},
  date = {2021-09-29},
  eprint = {2109.14279},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.14279},
  url = {http://arxiv.org/abs/2109.14279},
  urldate = {2025-03-25},
  abstract = {Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. The code to reproduce our results can be found at https://github.com/valeoai/LOST.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/juntangwang/Zotero/storage/DMPPTKAF/Siméoni et al. - 2021 - Localizing objects with self-supervised transformers and no labels.pdf;/Users/juntangwang/Zotero/storage/LX8G8WIW/2109.html}
}

@misc{stanford-ribonanza-rna-folding,
  title = {Stanford Ribonanza {{RNA}} Folding},
  author = {Das, Rhiju and He, Shujun and Huang, Rui and Townley, Jill and Kretsch, Rachael and Karagianes, Thomas and Nicol, John and Nye, Grace and Choe, Christian and Romano, Jonathan and Demkin, Maggie and Reade, Walter and {players}, Eterna},
  date = {2023},
  url = {https://kaggle.com/competitions/stanford-ribonanza-rna-folding},
  keywords = {notion}
}

@online{touvronDeiTIIIRevenge2022,
  title = {{{DeiT III}}: {{Revenge}} of the {{ViT}}},
  shorttitle = {{{DeiT III}}},
  author = {Touvron, Hugo and Cord, Matthieu and Jégou, Hervé},
  date = {2022-04-14},
  eprint = {2204.07118},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.07118},
  url = {http://arxiv.org/abs/2204.07118},
  urldate = {2025-03-25},
  abstract = {A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure builds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations, closer to the practice in self-supervised learning. Our evaluations on Image classification (ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better baselines for recent self-supervised approaches demonstrated on ViT.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/juntangwang/Zotero/storage/DTCFVLKC/Touvron et al. - 2022 - DeiT III Revenge of the ViT.pdf;/Users/juntangwang/Zotero/storage/X7YYF4AU/2204.html}
}

@online{zhangDINODETRImproved2022,
  title = {{{DINO}}: {{DETR}} with Improved {{DeNoising}} Anchor Boxes for End-to-End Object Detection},
  shorttitle = {{{DINO}}},
  author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
  date = {2022-07-11},
  eprint = {2203.03605},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.03605},
  url = {http://arxiv.org/abs/2203.03605},
  urldate = {2025-03-25},
  abstract = {We present DINO (\textbackslash textbf\{D\}ETR with \textbackslash textbf\{I\}mproved de\textbackslash textbf\{N\}oising anch\textbackslash textbf\{O\}r boxes), a state-of-the-art end-to-end object detector. \% in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves \$49.4\$AP in \$12\$ epochs and \$51.3\$AP in \$24\$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of \$\textbackslash textbf\{+6.0\}\$\textbackslash textbf\{AP\} and \$\textbackslash textbf\{+2.7\}\$\textbackslash textbf\{AP\}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \textbackslash texttt\{val2017\} (\$\textbackslash textbf\{63.2\}\$\textbackslash textbf\{AP\}) and \textbackslash texttt\{test-dev\} (\textbackslash textbf\{\$\textbackslash textbf\{63.3\}\$AP\}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \textbackslash url\{https://github.com/IDEACVR/DINO\}.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/juntangwang/Zotero/storage/VHSHSQ5Y/Zhang et al. - 2022 - DINO DETR with improved DeNoising anchor boxes for end-to-end object detection.pdf;/Users/juntangwang/Zotero/storage/I4S3BQ3E/2203.html}
}

@online{zhuTransformersNormalization2025,
  title = {Transformers without Normalization},
  author = {Zhu, Jiachen and Chen, Xinlei and He, Kaiming and LeCun, Yann and Liu, Zhuang},
  date = {2025-03-13},
  eprint = {2503.10622},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.10622},
  url = {http://arxiv.org/abs/2503.10622},
  urldate = {2025-03-25},
  abstract = {Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation \$DyT(\$x\$) = \textbackslash tanh(\textbackslash alpha \$x\$)\$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, \$S\$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/juntangwang/Zotero/storage/6YGFD7JY/Zhu et al. - 2025 - Transformers without normalization.pdf;/Users/juntangwang/Zotero/storage/XZ9K6VC7/2503.html}
}
